{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Analysis\n",
    "Detailed evaluation of the trained mood classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load saved test metrics\n",
    "with open('../models/test_metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print('Overall Test Metrics:')\n",
    "print(f\"Loss: {metrics['test_loss']:.4f}\")\n",
    "print(f\"F1 Micro: {metrics['test_f1_micro']:.4f}\")\n",
    "print(f\"F1 Macro: {metrics['test_f1_macro']:.4f}\")\n",
    "\n",
    "print('\\nPer-Mood Metrics:')\n",
    "for mood, mood_metrics in metrics['per_mood_metrics'].items():\n",
    "    print(f\"{mood}:\")\n",
    "    print(f\"  Precision: {mood_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {mood_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1: {mood_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Per-Mood Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract metrics for visualization\n",
    "moods = list(metrics['per_mood_metrics'].keys())\n",
    "precisions = [metrics['per_mood_metrics'][m]['precision'] for m in moods]\n",
    "recalls = [metrics['per_mood_metrics'][m]['recall'] for m in moods]\n",
    "f1s = [metrics['per_mood_metrics'][m]['f1'] for m in moods]\n",
    "\n",
    "x = np.arange(len(moods))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(x - width, precisions, width, label='Precision', color='steelblue')\n",
    "ax.bar(x, recalls, width, label='Recall', color='orange')\n",
    "ax.bar(x + width, f1s, width, label='F1', color='green')\n",
    "\n",
    "ax.set_xlabel('Mood')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Per-Mood Performance Metrics')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(moods, rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/per_mood_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model for Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "MODEL_PATH = '../models/distilbert-mood-classifier'\n",
    "MOODS = ['work', 'date', 'quick_bite', 'budget', 'family', 'late_night', 'celebration']\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print('Model loaded for inference testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def predict_mood(text, threshold=0.5):\n",
    "    \"\"\"Predict mood probabilities for a review\"\"\"\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    \n",
    "    mood_probs = {mood: float(prob) for mood, prob in zip(MOODS, probs)}\n",
    "    predicted_moods = [mood for mood, prob in mood_probs.items() if prob > threshold]\n",
    "    \n",
    "    return mood_probs, predicted_moods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Sample Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "test_reviews = [\n",
    "    \"Perfect spot to work on my laptop. Great WiFi and plenty of power outlets. Coffee is excellent too.\",\n",
    "    \"Most romantic dinner I've ever had. Dim lighting, intimate atmosphere, and the service was impeccable.\",\n",
    "    \"Quick lunch spot. Got my sandwich in 3 minutes. Perfect for busy workdays.\",\n",
    "    \"Amazing value! Huge portions for the price. Very affordable and tasty.\",\n",
    "    \"Brought the whole family. Kids menu was great and staff were very accommodating with the little ones.\",\n",
    "    \"Open until 2 AM! Perfect for late night cravings after a concert.\",\n",
    "    \"Celebrated my birthday here. Fancy atmosphere and they brought out a special dessert with candles.\"\n",
    "]\n",
    "\n",
    "print('Testing model on sample reviews:\\n')\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    print(f\"Review {i}: {review}\")\n",
    "    mood_probs, predicted = predict_mood(review)\n",
    "    \n",
    "    print(f\"Predicted moods: {', '.join(predicted) if predicted else 'None'}\")\n",
    "    print(\"Top 3 probabilities:\")\n",
    "    sorted_moods = sorted(mood_probs.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    for mood, prob in sorted_moods:\n",
    "        print(f\"  {mood}: {prob:.3f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load labeled data\n",
    "with open('../data/labeled/labeled_reviews.json', 'r') as f:\n",
    "    labeled_data = json.load(f)\n",
    "\n",
    "print(f'Total labeled reviews: {len(labeled_data)}')\n",
    "\n",
    "# Count mood occurrences\n",
    "all_moods = []\n",
    "for review in labeled_data:\n",
    "    all_moods.extend(review['moods'])\n",
    "\n",
    "mood_counts = Counter(all_moods)\n",
    "\n",
    "print('\\nMood distribution in training data:')\n",
    "for mood, count in mood_counts.most_common():\n",
    "    percentage = (count / len(labeled_data)) * 100\n",
    "    print(f'{mood}: {count} ({percentage:.1f}%)')\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "moods_sorted = sorted(mood_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "moods_names = [m[0] for m in moods_sorted]\n",
    "moods_values = [m[1] for m in moods_sorted]\n",
    "\n",
    "plt.bar(moods_names, moods_values, color='steelblue')\n",
    "plt.xlabel('Mood')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mood Distribution in Labeled Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/mood_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze multi-label patterns\n",
    "label_counts = [len(review['moods']) for review in labeled_data]\n",
    "label_count_dist = Counter(label_counts)\n",
    "\n",
    "print('Number of labels per review:')\n",
    "for num_labels, count in sorted(label_count_dist.items()):\n",
    "    percentage = (count / len(labeled_data)) * 100\n",
    "    print(f'{num_labels} labels: {count} reviews ({percentage:.1f}%)')\n",
    "\n",
    "avg_labels = np.mean(label_counts)\n",
    "print(f'\\nAverage labels per review: {avg_labels:.2f}')\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(label_count_dist.keys(), label_count_dist.values(), color='coral')\n",
    "plt.xlabel('Number of Moods per Review')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Label Counts')\n",
    "plt.xticks(list(label_count_dist.keys()))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/label_count_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find reviews where model struggled (low confidence or wrong predictions)\n",
    "problematic_reviews = []\n",
    "\n",
    "for review in labeled_data[:50]:  # Sample first 50\n",
    "    text = review['review_text']\n",
    "    true_moods = set(review['moods'])\n",
    "    \n",
    "    mood_probs, predicted_moods = predict_mood(text)\n",
    "    predicted_set = set(predicted_moods)\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap = len(true_moods.intersection(predicted_set))\n",
    "    total = len(true_moods.union(predicted_set))\n",
    "    \n",
    "    if total > 0:\n",
    "        accuracy = overlap / total\n",
    "    else:\n",
    "        accuracy = 0\n",
    "    \n",
    "    if accuracy < 0.5:  # Low accuracy\n",
    "        problematic_reviews.append({\n",
    "            'text': text[:100] + '...',\n",
    "            'true': list(true_moods),\n",
    "            'predicted': list(predicted_set),\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "\n",
    "print(f'Found {len(problematic_reviews)} problematic reviews (accuracy < 0.5):\\n')\n",
    "for i, rev in enumerate(problematic_reviews[:5], 1):\n",
    "    print(f\"{i}. {rev['text']}\")\n",
    "    print(f\"   True: {rev['true']}\")\n",
    "    print(f\"   Predicted: {rev['predicted']}\")\n",
    "    print(f\"   Accuracy: {rev['accuracy']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "summary = f\"\"\"\n",
    "MODEL EVALUATION SUMMARY\n",
    "{'='*60}\n",
    "\n",
    "Dataset Statistics:\n",
    "- Total labeled reviews: {len(labeled_data)}\n",
    "- Average moods per review: {avg_labels:.2f}\n",
    "- Most common mood: {mood_counts.most_common(1)[0][0]} ({mood_counts.most_common(1)[0][1]} occurrences)\n",
    "\n",
    "Model Performance:\n",
    "- Test F1 (Macro): {metrics['test_f1_macro']:.4f}\n",
    "- Test F1 (Micro): {metrics['test_f1_micro']:.4f}\n",
    "- Test Loss: {metrics['test_loss']:.4f}\n",
    "\n",
    "Best Performing Moods:\n",
    "\"\"\"\n",
    "\n",
    "# Sort moods by F1 score\n",
    "sorted_moods = sorted(\n",
    "    metrics['per_mood_metrics'].items(),\n",
    "    key=lambda x: x[1]['f1'],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for mood, mood_metrics in sorted_moods[:3]:\n",
    "    summary += f\"- {mood}: F1 = {mood_metrics['f1']:.4f}\\n\"\n",
    "\n",
    "summary += \"\\nLowest Performing Moods:\\n\"\n",
    "for mood, mood_metrics in sorted_moods[-3:]:\n",
    "    summary += f\"- {mood}: F1 = {mood_metrics['f1']:.4f}\\n\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open('../models/evaluation_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print('\\nEvaluation summary saved to models/evaluation_summary.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}